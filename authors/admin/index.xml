<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bargav Jayaraman</title>
    <link>https://bargavjayaraman.github.io/authors/admin/</link>
    <description>Recent content on Bargav Jayaraman</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://bargavjayaraman.github.io/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Xbox One X HDD Loading Test</title>
      <link>https://bargavjayaraman.github.io/post/xbox_one_x_hdd_loading_test/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/post/xbox_one_x_hdd_loading_test/</guid>
      <description>

&lt;p&gt;With the advent of 4K UHD and HDR games, it is not uncommon nowadays that your Xbox One X stock HDD of 1 TB gets filled quickly with just a handful of games. And my case is no different, within one year of purchasing the console, I already ran out of hard disk space. So I did what any gamer would do in my situation; I ordered a &lt;a href=&#34;https://www.bestbuy.com/site/seagate-expansion-4tb-external-usb-3-0-portable-hard-drive-black/4820200.p?skuId=4820200&#34; target=&#34;_blank&#34;&gt;4TB external hard drive&lt;/a&gt; [Yay!]. Now that I planned to transfer some games from my internal HDD to the external HDD, curiosity plagued the researcher in me! Is the internal HDD faster at loading games than the external HDD? Or is it the other way around? Both schools of thoughts have their own reasoning. On one hand internal HDD is, well, internal. It uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Serial_ATA&#34; target=&#34;_blank&#34;&gt;SATA 3&lt;/a&gt; connection supporting upto 600 MB/s of bandwidth. On the other hand, external HDD uses &lt;a href=&#34;https://en.wikipedia.org/wiki/USB_3.0&#34; target=&#34;_blank&#34;&gt;USB 3.0&lt;/a&gt; connection supporting upto 625 MB/s of bandwidth. Both the hard drives have rotation speed of 5400 RPM, so it is still an apple to apple comparison.&lt;/p&gt;

&lt;p&gt;Note that this idea of comparing the loading speeds of internal and external HDDs is not novel. There are already many videos and blogs on this topic - be it for computers or consoles. What&amp;rsquo;s new in this blog is to evaluate this in a [&lt;em&gt;cough&lt;/em&gt;] scientific way - using hypothesis testing. Let&amp;rsquo;s get into a [somewhat] scientific evaluation!&lt;/p&gt;

&lt;h2 id=&#34;hypothesis-testing-on-game-loading-task&#34;&gt;Hypothesis testing on game loading task&lt;/h2&gt;

&lt;p&gt;Game loading time is measured (in &lt;em&gt;mm:ss&lt;/em&gt;) as the time taken to launch the game - from clicking on the game till the point where the player gets the control of the playable character, as shown in this &lt;a href=&#34;https://www.youtube.com/watch?v=MQ_pDLMI650&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;. I recorded the loading time for 10 AAA title games, with the exception of Forza Motorsport 7, where I recorded till the point where the main menu appears. Launching a race involves lot of manual selection process, which could add more variability in recording time.&lt;/p&gt;

&lt;p&gt;The null hypothesis is $H_0:$ &lt;em&gt;&amp;ldquo;game loading time of internal HDD is same as that of external HDD&amp;rdquo;&lt;/em&gt;, and the alternate hypothesis is $H_a:$ &lt;em&gt;&amp;ldquo;game loading time of internal HDD is not same as that of external HDD&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;experimental-observations&#34;&gt;Experimental observations&lt;/h2&gt;

&lt;p&gt;The game loading times are reported in the table below.
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Game Title&lt;/th&gt; &lt;th&gt;Internal HDD Time &lt;br&gt;(&lt;i&gt;mm:ss&lt;/i&gt;)&lt;/th&gt; &lt;th&gt;External HDD Time &lt;br&gt;(&lt;i&gt;mm:ss&lt;/i&gt;)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Assassin&amp;rsquo;s Creed Origins&lt;/td&gt; &lt;td&gt;01:48.50&lt;/td&gt; &lt;td&gt;01:51.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Tomb Raider&lt;/td&gt; &lt;td&gt;00:50.95&lt;/td&gt; &lt;td&gt;00:41.78&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Rise of Tomb Raider&lt;/td&gt; &lt;td&gt;02:32.14&lt;/td&gt; &lt;td&gt;02:15.54&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Shadow of Tomb Raider&lt;/td&gt; &lt;td&gt;01:36.63&lt;/td&gt; &lt;td&gt;01:24.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Halo Master Chief Collection&lt;/td&gt; &lt;td&gt;00:57.29&lt;/td&gt; &lt;td&gt;00:46.89&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Batman: Arkham Assylum&lt;/td&gt; &lt;td&gt;01:09.88&lt;/td&gt; &lt;td&gt;01:03.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Batman: Arkham City&lt;/td&gt; &lt;td&gt;01:13.94&lt;/td&gt; &lt;td&gt;01:12.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Batman: Arkham Knight&lt;/td&gt; &lt;td&gt;01:39.64&lt;/td&gt; &lt;td&gt;01:36.14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Forza Horizon 4&lt;/td&gt; &lt;td&gt;02:33.74&lt;/td&gt; &lt;td&gt;02:04.31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Forza Motorsport 7&lt;/td&gt; &lt;td&gt;01:00.00&lt;/td&gt; &lt;td&gt;00:50.69&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;Since there is a one-to-one correspondence of each game, we can use &lt;a href=&#34;https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/t-test/&#34; target=&#34;_blank&#34;&gt;paired sample T-test&lt;/a&gt;.
We can calculate the $t$-value for the paired sample T-test using a simple formula:&lt;/p&gt;

&lt;p&gt;$t-value = \frac{D_1 / N}{\sqrt{\frac{D_2 - D_1^2 / N}{N(N-1)}}}$&lt;/p&gt;

&lt;p&gt;Where $N$ is the total number of games ($N = 10$), $D_1$ is the total difference between loading times of internal and external HDDs ($D_1 = 96.71$) and $D_2$ is the total squared difference between loading times of internal and external HDDs ($D_2 = 1641.4153$). The calculated $t$-value comes out to be 3.4526 which is greater than the threshold $t$-value of 3.250 for a significance threshold of 0.005. Hence we can reject the null hypothesis $H_0$ with 99.5% confidence and conclude that [&lt;em&gt;my&lt;/em&gt;] external HDD is faster than the internal HDD of [&lt;em&gt;my&lt;/em&gt;] Xbox One X.&lt;/p&gt;

&lt;h2 id=&#34;disclaimer-on-the-validity-of-results&#34;&gt;Disclaimer on the validity of results&lt;/h2&gt;

&lt;p&gt;The above results are for a single loading of each game, and the loading time might vary by a few seconds across multiple runs. Ideally, average loading time should be used for such experiments. However, it should be noted that if the same game is loaded multiple times, the consequent loading will be faster due to &lt;em&gt;memory caching&lt;/em&gt;. To avoid caching, games should be loaded in an alternative fashion. Finally, it is important to put a disclaimer that the reported results are for my hardware, and these results &lt;em&gt;might&lt;/em&gt; vary for others. So take it with a grain of salt! :P&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Keeping the above disclaimer in mind, I conclude that my external HDD loads games faster than the stock HDD of my Xbox One X. This benefit might be attributed to the extra 25 MB/s bandwidth of USB 3.0 interface used by the external HDD.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>https://bargavjayaraman.github.io/publication/evaluating-dpml/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/evaluating-dpml/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/JAGhqbY_U50&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization</title>
      <link>https://bargavjayaraman.github.io/publication/dp-erm/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/dp-erm/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/rwyWiDyVmjE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>https://bargavjayaraman.github.io/post/evaluating-dpml-results/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/post/evaluating-dpml-results/</guid>
      <description>

&lt;p&gt;With the recent advancements in composition of differential private mechanisms, the research community has been able to achieve meaningful deep learning with privacy budgets in single digits&lt;sup&gt;&lt;a href=&#34;#myfootnote1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The present notions of R&amp;#x00E8;nyi differential privacy (RDP) and Gaussian differential privacy (GDP)&lt;sup&gt;&lt;a href=&#34;#myfootnote2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; are considered state-of-the-art for providing tight composition. But the central question that remains to be answered is: how private are these methods in practice? In this blog post, we answer this question by empirically evaluating the privacy leakage of differential private neural networks via membership inference attacks. This work appeared in &lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity19&#34; target=&#34;_blank&#34;&gt;USENIX Security&amp;rsquo;19&lt;/a&gt; (full manuscript can be found &lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/evaluatingdp.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;differential-private-training&#34;&gt;Differential private training&lt;/h2&gt;

&lt;p&gt;We train two-layer neural network models using a training procedure similar to the popular &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/2976749.2978318&#34; target=&#34;_blank&#34;&gt;DPSGD&lt;/a&gt; procedure. The training and test sets consist of seperate 10,000 instances randomly sampled from the &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34; target=&#34;_blank&#34;&gt;CIFAR-100&lt;/a&gt; data set.&lt;/p&gt;

&lt;p&gt;Figure below shows the accuracy loss of private models trained with na&amp;#x00EF;ve composition (NC) and R&amp;#x00E8;nyi differential privacy (RDP) with respect to a non-private model. As expected, model trained with RDP achieves much better utility when compared to the model trained with NC. To give a comparison, model trained with RDP achieves 53% accuracy loss at \(\epsilon = 10\), whereas the model trained with NC achieves the same utility at \(\epsilon = 500\). Due to the tighter composition, RDP mechanism adds much lesser noise when compared to NC mechanism for the same privacy budget. This is great, but what about the privacy leakage?






&lt;figure&gt;

&lt;img src=&#34;img/acc_loss.jpg&#34; &gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;privacy-comes-at-a-cost&#34;&gt;Privacy comes at a cost&lt;/h2&gt;

&lt;p&gt;For evaluating the privacy leakage, we implement the membership inference attack of &lt;a href=&#34;https://ieeexplore.ieee.org/document/8429311&#34; target=&#34;_blank&#34;&gt;Yeom et al&lt;/a&gt; and use their membership advantage metric, which is given as the difference between true positive rate (TPR) and false positive rate (FPR) of detecting whether a given instance is a part of the training set. This metric lies between 0 and 1, where 0 signifies no privacy leakage. As the figure below depicts, there is a clear trade-off between privacy and utility. While RDP mechanism achieves higher utility, it also suffers from higher privacy leakage. The attack achieves around 0.40 membership advantage score against model trained with RDP at \(\epsilon = 1000\), with a positive predictive value (PPV) of 74%. While this is lesser than the privacy leakage of non-private model (highlighted in the figure below), it is still not an acceptable amount of privacy leakage in practice. On the other hand, the model has almost no utility at lower privacy budgets where the privacy leakage is low.






&lt;figure&gt;

&lt;img src=&#34;img/priv_leak.jpg&#34; &gt;


&lt;/figure&gt;

A more interesting observation is that we only have tight theoretical worst case guarantees on membership advantage for \(\epsilon &amp;lt; 1\), at which point the models neither have any utility nor have any empirical privacy leakage. While the attacks only give a lower bound on the privacy leakage, the huge gap between the theoretical upper bound and the empirical lower bound suggests that there could be stronger attacks  in practice.&lt;/p&gt;

&lt;h2 id=&#34;leakage-is-amplified-across-multiple-runs&#34;&gt;Leakage is amplified across multiple runs&lt;/h2&gt;

&lt;p&gt;While we have shown above that the membership inference attack can be effective against a model trained with RDP at \(\epsilon = 1000\), things get worse when the attacker is allowed to run the attack multiple times. More specifically, the attacker gets more and more confident in predicting the membership of the individual records which are repeatedly identified across multiple runs. As the figure below shows, the attacker can identify almost a quarter of the training records with more than 82% PPV across five runs.






&lt;figure&gt;

&lt;img src=&#34;img/multi_run.jpg&#34; &gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The differential privacy research community has come a long way to realize practical mechanisms for deep learning. However, as shown in our work, we still require significant improvements to achieve meaningful utility for privacy budgets where we have strong theoretical guarantees. Concurrently, the huge gap between the empirical privacy leakage and the theoretical bounds opens the possibility for more powerful inference attacks in practice.&lt;/p&gt;

&lt;h2 id=&#34;what-s-more-in-the-paper&#34;&gt;What&amp;rsquo;s more in the paper&lt;/h2&gt;

&lt;p&gt;While we only discussed selected results in this blog post, the &lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/evaluatingdp.pdf&#34; target=&#34;_blank&#34;&gt;full paper&lt;/a&gt; has more experimental results across different settings as listed below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Results on Purchase-100 data set, derived from &lt;a href=&#34;https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data&#34; target=&#34;_blank&#34;&gt;Kaggle&lt;/a&gt; website.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Results for logistic regression model.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Membership inference attack of &lt;a href=&#34;https://ieeexplore.ieee.org/document/7958568&#34; target=&#34;_blank&#34;&gt;Shokri et al.&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Attribute inference attack of &lt;a href=&#34;https://ieeexplore.ieee.org/document/8429311&#34; target=&#34;_blank&#34;&gt;Yeom et al&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;myfootnote1&#34;&gt;1&lt;/a&gt;: with some caveats such as model pre-training and adding less noise at each private training epoch with early stopping when the privacy budget exceeds the limit.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;myfootnote2&#34;&gt;2&lt;/a&gt;: GDP wasn&amp;rsquo;t published at the time our paper came, and hence we donot consider it in our evaluation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Vague Words and Phrases in Requirements Documents in a Multilingual Environment</title>
      <link>https://bargavjayaraman.github.io/publication/vagueness/</link>
      <pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/vagueness/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Decentralized Certificate Authorities</title>
      <link>https://bargavjayaraman.github.io/publication/dca/</link>
      <pubDate>Sun, 11 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/dca/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Aggregating Private Sparse Learning Models Using Multi-Party Computation</title>
      <link>https://bargavjayaraman.github.io/publication/model-aggregation/</link>
      <pubDate>Sun, 04 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/model-aggregation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Preserving String Matching for Cloud Computing</title>
      <link>https://bargavjayaraman.github.io/publication/searchable-encryption/</link>
      <pubDate>Mon, 29 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/searchable-encryption/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://bargavjayaraman.github.io/authors/admin/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/authors/admin/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m currently doing PhD in Computer Science at School of Engineering and Applied Science, University of Virginia. I am advised by &lt;a href=&#34;https://www.cs.virginia.edu/~evans/&#34; target=&#34;_blank&#34;&gt;Professor David Evans&lt;/a&gt; and my area of research is broadly in &lt;a href=&#34;https://oblivc.org/ppml/&#34; target=&#34;_blank&#34;&gt;Privacy Preserving Machine Learning&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
