<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bargav Jayaraman</title>
    <link>https://bargavjayaraman.github.io/authors/admin/</link>
    <description>Recent content on Bargav Jayaraman</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 May 2025 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://bargavjayaraman.github.io/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Permissioned LLMs: Enforcing Access Control in Large Language Models</title>
      <link>https://bargavjayaraman.github.io/publication/permissioned-llms/</link>
      <pubDate>Thu, 29 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/permissioned-llms/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dèjá Vu Memorization in Vision-Language Models</title>
      <link>https://bargavjayaraman.github.io/publication/dejavu_vlms/</link>
      <pubDate>Sat, 03 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/dejavu_vlms/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Attribute Inference Attacks Pose Distribution Inference Risk to Models</title>
      <link>https://bargavjayaraman.github.io/post/revisiting-ai/</link>
      <pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/post/revisiting-ai/</guid>
      <description>

&lt;p&gt;&lt;center&gt;
&lt;img alt=&#34;&#34; src=&#34;img/ai.png&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Attribute inference&lt;/em&gt; attacks have been shown by prior works to pose privacy threat against ML models. However, these works assume the knowledge of the training distribution and we show that in such cases these attacks do no better than a data imputataion attack that does not have access to the model. We explore the attribute inference risks in the cases where the adversary has limited or no prior knowledge of the training distribution and show that our white-box attribute inference attack (that uses neuron activations to infer the unknown sensitive attribute) surpasses imputation in these data constrained cases. This attack uses the training distribution information leaked by the model, and thus poses privacy risk when the distribution is private.&lt;/p&gt;

&lt;h2 id=&#34;prior-attribute-inference-attacks-do-not-pose-privacy-risk&#34;&gt;Prior Attribute Inference Attacks Do Not Pose Privacy Risk&lt;/h2&gt;

&lt;p&gt;Prior works in attribute inference have mainly considered black-box access to the machine learning model and show successful attribute inference (in terms of attack accuracy) in the case where the adversary has access to the underlying training distribution. Our experiments show that in such cases even an imputation adversary, without access to the model, can achieve high inference accuracy, as shown in the table below:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Census (Race)&lt;/th&gt;
&lt;th&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Texas-100X (Ethnicity)&lt;/th&gt;
&lt;th&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Predict Most Common&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.78&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.72&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Imputation Attack&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.82&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.72&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Yeom et al. Attack&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.65&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.58&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Mehnaz et al. Attack&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.06&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.60&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WCAI (Our version of Yeom)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.83&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.74&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;Comparing attribute inference accuracy of attacks&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Thus, these attribute inference attacks seem to not pose any significant privacy risk as the adversary can have similar attack success without access to the model.&lt;/p&gt;

&lt;h2 id=&#34;sensitive-value-inference&#34;&gt;Sensitive Value Inference&lt;/h2&gt;

&lt;p&gt;Attribute inference risk is inherently asymmetric&amp;mdash; identifying a record with minority attribute value (such as &lt;em&gt;Hispanic&lt;/em&gt; ethnicity) does not have the same risk as identifying a record with majority attribute value (such as &lt;em&gt;Non-Hispanic&lt;/em&gt; ethnicity). Accuracy metric does not capture this. Moreover, attribute inference definition considered by prior works also fails to distinguish these cases. We propose studying a fine-grained version of attribute inference, called &lt;em&gt;sensitive value inference&lt;/em&gt;, that considers the attack success in inferring a particular sensitive attribute outcome.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img alt=&#34;&#34; src=&#34;img/svi.png&#34; width=&#34;80%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;We measure the attack success by evaluating the positive predictive value (PPV) of the inference attack in predicting the top-k candidate records with the sensitive outcome. The PPV values are between 0 and 1, where a higher value denotes a greater attack precision.&lt;/p&gt;

&lt;h2 id=&#34;the-neuron-output-attack&#34;&gt;The Neuron Output Attack&lt;/h2&gt;

&lt;p&gt;Our novel neuron output based white-box attack finds the neurons that are most correlated with the sensitive value. For this attack, the adversary selects records from a hold-out set, sets the unknown target attribute to the sensitive value, and queries the model. The adversary then identifies the set of neurons that have higher activations on average for the records with the sensitive value as the ground-truth. The adversary then uses the aggregate output of these neurons to identify the candidate records with sensitive value.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img alt=&#34;&#34; src=&#34;img/wb.png&#34; width=&#34;75%&#34;&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;model-leaks-distribution-information&#34;&gt;Model Leaks Distribution Information&lt;/h2&gt;

&lt;p&gt;In our experiments, we vary the distribution available to the adversary and also the amount of data from the respective distribution the adversary has to train the inference attack. When the adversary has access to &amp;gt;5000 records from the training distribition (not the same as the training set records), imputataion outperforms all the attribute inference attacks (incuding our white-box neuron output attack). As we decrease the known set size to 500 and 50, the imputation PPV decreases drastically whereas our neuron output attack continues to achieve high PPV. Thus the attack is able to take advantage of the training distribution information leaked by the model. The figure below depicts the case where the adversary has 500 records from the training distribution, and as shown, the neuron output attack surpasses the imputataion.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img alt=&#34;&#34; src=&#34;img/img2.png&#34; width=&#34;100%&#34;&gt;
&lt;div class=&#34;caption&#34;&gt;
Neurons correlated to Hispanic ethnicity for a neural network model trained on Texas-100X data set.
&lt;/div&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;We observe similar trend across different distribution settings and across different data sets. Detailed results can be found in the paper.&lt;/p&gt;

&lt;h2 id=&#34;differential-privacy-doesn-t-mitigate-the-risk&#34;&gt;Differential Privacy Doesn&amp;rsquo;t Mitigate the Risk&lt;/h2&gt;

&lt;p&gt;Prior works have claimed that attribute inference attacks cannot work in the cases where membership inference attacks do not succeed. Hence, the prior works&amp;rsquo; claim is that differential privacy mechanisms, that defend against membership inference attacks, also defend against attribute inference attacks. This is based on the &lt;em&gt;attribute advantage&lt;/em&gt; metric of Yeom et al. that shows that the difference between the &lt;em&gt;accuracy&lt;/em&gt; of inference attack across training and non-training set is bounded by differential privacy. We agree that this is true, as we shown in our experiment results in Table 2 below where the PPV of the neuron output attack is similar across both train and test sets. However, our &lt;em&gt;attribute advantage&lt;/em&gt; metric measures the gap between the attack PPV when the adversary has access to the model (i.e., neuron output attack) versus when the adversary does not have model access (i.e., imputataion). As shown in the table below, this is not bounded by differential privacy as the neuron output attack PPV remains more or less the same with or without differential privacy.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Without DP&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;With DP&lt;/th&gt;
&lt;th&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Train Set&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Test Set&lt;/th&gt;
&lt;th&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Imputation Attack&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.62 $\pm$ 0.05&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.62 $\pm$ 0.05&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.62 $\pm$ 0.05&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.63 $\pm$ 0.02&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Neuron Output Attack&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.49 $\pm$ 0.02&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.49 $\pm$ 0.03&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.49 $\pm$ 0.03&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.48 $\pm$ 0.02&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;Impact of Differential Privacy (DP) on the PPV of attacks (see table in paper for error margins). Results show the PPV of attacks in predicting top-100 candidate records.&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Since the risk is due to the model leaking distribution information, it is not mitigated by differential privacy noise.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We show that the attribute inference attacks take advantage of the model leaking sensitive information about the underlying training distribution as opposed to leaking information about individual training records. While this is often considered by researchers to be &lt;strong&gt;not&lt;/strong&gt; a privacy risk since the distribution statistics are supposed to be public knowledge, we argue that when the distribution itself is a private information then any such disclosure poses a severe privacy risk. Existing defenses, such as training the model with differential privacy mechanisms, does not mitigate this distribution privacy risk.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Full paper:&lt;/strong&gt; Bargav Jayaraman and David Evans. &lt;a href=&#34;https://arxiv.org/abs/2209.01292&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Are Attribute Inference Attacks Just Imputation?&lt;/em&gt;&lt;/a&gt; (&lt;a href=&#34;https://arxiv.org/abs/2209.01292&#34; target=&#34;_blank&#34;&gt;arXiv&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href=&#34;https://github.com/bargavj/EvaluatingDPML&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;https://github.com/bargavj/EvaluatingDPML&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Talk Video:&lt;/strong&gt; &lt;a href=&#34;https://youtu.be/iLy0C5DK2T8&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;https://youtu.be/iLy0C5DK2T8&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are Attribute Inference Attacks Just Imputation?</title>
      <link>https://bargavjayaraman.github.io/publication/revisiting-ai/</link>
      <pubDate>Fri, 02 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/revisiting-ai/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Combing for Credentials: Active Pattern Extraction from Smart Reply</title>
      <link>https://bargavjayaraman.github.io/publication/smart-reply-attack/</link>
      <pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/smart-reply-attack/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Defense Against Attribute Inference</title>
      <link>https://bargavjayaraman.github.io/post/ai_defense_results/</link>
      <pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/post/ai_defense_results/</guid>
      <description>

&lt;h2 id=&#34;jun-27-2022-vanilla-model-training&#34;&gt;[Jun 27, 2022] Vanilla Model Training&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;predicting-hispanic-ethnicity-from-texas-100x-i&#34;&gt;Predicting &lt;em&gt;Hispanic&lt;/em&gt; ethnicity from Texas-100X (I)&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Threat Setting:&lt;/strong&gt; &lt;br&gt;
Training distirbution is general data set distribution except the most populous hospitals.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;u&gt;Adversarial Knowledge&lt;/u&gt;:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;High:&lt;/em&gt; Adversary knows all but one record from training set.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Medium:&lt;/em&gt; Adversary knows the training distribution (i.e. can sample records with no intersection with training set).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Low:&lt;/em&gt; Adversary knows a skewed distribution (that has records from hospitals with highest number of patients).&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;img/texas-100x.jpg&#34; width=&#34;100%&#34;&gt;&lt;/center&gt;
&lt;center&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Patient records distribution in Texas-100X, sorted with respect to hospital population.&lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&#34;model-uses-race-as-a-training-feature&#34;&gt;Model uses &lt;em&gt;race&lt;/em&gt; as a training feature&lt;/h4&gt;

&lt;p&gt;As reported in the paper draft, the model trained on Texas-100X data set uses the race attribute as a feature which could have a positive correlation with the ethnicity attribute. Regardless of this, it is still useful to compare the attribute inference attacks with the imputation (IP) attack in this setting.&lt;/p&gt;

&lt;p&gt;&lt;details&gt;
    &lt;summary&gt;&lt;strong&gt;Hypothesis:&lt;/strong&gt; Passing WB$\cdot$IP as a feature to WB$\diamondsuit$IP&amp;rsquo;s decision tree should improve the attack.&lt;/summary&gt;
    &lt;strong&gt;Remark:&lt;/strong&gt; The result is a mixed bag. WB$\square$IP attack does better than WB$\diamondsuit$IP attack in many (but not all) cases. Still for most cases, WB$\square$IP doesn&amp;rsquo;t do better than WB$\cdot$IP.
&lt;/details&gt;&lt;/p&gt;

&lt;p&gt;&lt;details&gt;
    &lt;summary&gt;&lt;strong&gt;Hypothesis:&lt;/strong&gt; There should be no gap between attribute inference and imputation with knowledge of train set.&lt;/summary&gt;
    &lt;strong&gt;Remark:&lt;/strong&gt; There is a considerable gap, even though this gap is reduced when imputation is trained with class label.
&lt;/details&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;WB$\cdot$IP multiplies the outputs of WB and IP &lt;br&gt;
WB$\diamondsuit$IP uses a decision tree model to combine WB and IP &lt;br&gt;
WB$\square$IP uses a decision tree model to combine WB, IP and WB$\cdot$IP &lt;br&gt;
IP$^\dagger$ is the imputation with access to class label&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;u&gt;Candidate Sets&lt;/u&gt;:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Train:&lt;/em&gt; Subset of 10,000 candidate records drawn from the training set.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Test:&lt;/em&gt; Subset of 10,000 candidate records randomly sampled from the training distribution.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;OOD:&lt;/em&gt; Subset of 10,000 candidate records randomly sampled from a distribution different from the training distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Test)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Random&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;79 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;79 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;75 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;76 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;83 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;79 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;74 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;75 $\pm$ 7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;79 $\pm$ 8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;78 $\pm$ 8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;79 $\pm$ 9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;74 $\pm$ 8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;83 $\pm$ 9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;77 $\pm$ 9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Table 1:&lt;/strong&gt; PPV (%) for predicting top-100 records on Texas-100X. Model is trained on general distribution.&lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&#34;model-doesn-t-have-access-to-race&#34;&gt;Model doesn&amp;rsquo;t have access to &lt;em&gt;race&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;Here we study the impact of removing race from the model training. Since race and ethnicity have implicit correlation, it would be more realistic to assume the adversary trying to infer ethnicity would not know the race of the query record.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Test)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Random&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;44 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47 $\pm$ 8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;39 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;45 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;41 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;44 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;43 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;38 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;43 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;52 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;53 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;60 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;63 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;59 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;52 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51 $\pm$ 6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;49 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;49 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;50 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48 $\pm$ 8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;45 $\pm$ 7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Table 2:&lt;/strong&gt; PPV (%) for predicting top-100 records on Texas-100X. Model is trained on general distribution.&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; From the results in Table 2, we can see the impact of removing race attribute on ethnicity inference. The imputation PPV drops from ~80% to ~40%.&lt;/p&gt;

&lt;h4 id=&#34;results-on-out-of-distribution-candidate-set&#34;&gt;Results on out-of-distribution candidate set&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;High &lt;br&gt;(with Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Med &lt;br&gt;(with Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Low &lt;br&gt;(with Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;High &lt;br&gt;(w/o Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Med &lt;br&gt;(w/o Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Low &lt;br&gt;(w/o Race)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Random&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;70 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;71 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;37 $\pm$ 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;52 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;73 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;73 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;78 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;37 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;30 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;91 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;60 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;62 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;65 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;78 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;91 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;91 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;63 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;93 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;77 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;66 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;83 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;60 $\pm$ 9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;83 $\pm$ 2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;60 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Table 3:&lt;/strong&gt; PPV (%) for predicting top-100 records from out-of-distributon candidate set (Texas-100X). Model is trained on general distribution.&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;predicting-hispanic-ethnicity-from-texas-100x-ii&#34;&gt;Predicting &lt;em&gt;Hispanic&lt;/em&gt; ethnicity from Texas-100X (II)&lt;/h3&gt;

&lt;p&gt;Here we consider the scenario where the training data comes from skewed distribution, while an adversary (in low adversarial knowledge setting) might have access to the general pupulation distribution.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Threat Setting:&lt;/strong&gt; &lt;br&gt;
Training distirbution is a skewed distribution limited to records from most populous hospitals.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;u&gt;Adversarial Knowledge&lt;/u&gt;:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;High:&lt;/em&gt; Adversary knows all but one record from training set.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Medium:&lt;/em&gt; Adversary knows the training distribution (i.e. can sample records with no intersection with training set).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Low:&lt;/em&gt; Adversary knows the general data set distribution except the most populous hospitals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;model-uses-race-as-a-training-feature-1&#34;&gt;Model uses &lt;em&gt;race&lt;/em&gt; as a training feature&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Test)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Random&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;83 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;72 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;65 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;73 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;67 $\pm$ 6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;92 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;93 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;95 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;96 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;93 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;79 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;78 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;92 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;92 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;79 $\pm$ 7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;92 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;91 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;77 $\pm$ 7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;95 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;95 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;79 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;92 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;91 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;92 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;91 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;78 $\pm$ 6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Table 4:&lt;/strong&gt; PPV (%) for predicting top-100 records on Texas-100X. Model is trained on &lt;em&gt;skewed&lt;/em&gt; distribution.&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; As shown in Table 3, IP$^\dagger$ performs slightly worse than IP. Hence the class label seems to have a negative correlation with the Hispanic ethnicity in this setting where training records come from a skewed distribution.&lt;/p&gt;

&lt;h4 id=&#34;model-doesn-t-have-access-to-race-1&#34;&gt;Model doesn&amp;rsquo;t have access to &lt;em&gt;race&lt;/em&gt;&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Test)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Random&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;65 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;70 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;72 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;64 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;62 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;70 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;68 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;37 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56 $\pm$ 6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;60 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;59 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;83 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;67 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;65 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;60 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;60 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;59 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Table 5:&lt;/strong&gt; PPV (%) for predicting top-100 records on Texas-100X. Model is trained on &lt;em&gt;skewed&lt;/em&gt; distribution.&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; Similar to previous case, race&amp;rsquo;s correlation to ethnicity has a huge impact on the inference task.&lt;/p&gt;

&lt;h4 id=&#34;results-on-out-of-distribution-candidate-set-1&#34;&gt;Results on out-of-distribution candidate set&lt;/h4&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;High &lt;br&gt;(with Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Med &lt;br&gt;(with Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Low &lt;br&gt;(with Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;High &lt;br&gt;(w/o Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Med &lt;br&gt;(w/o Race)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Low &lt;br&gt;(w/o Race)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Random&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;76 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;76 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;37 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;43 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;41 $\pm$ 2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;75 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;80 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;42 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;38 $\pm$ 7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;52 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;50 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;53 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;53 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;63 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;74 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;74 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;45 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;59 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;75 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;83 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;90 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;56 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;53 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;61 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;78 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;74 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;44 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;81 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;78 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;85 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;45 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;45 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Table 6:&lt;/strong&gt; PPV (%) for predicting top-100 records from out-of-distributon candidate set (Texas-100X). Model is trained on &lt;em&gt;skewed&lt;/em&gt; distribution.&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;In all the above settings, the white-box attacks perform similar on both training and test candidates, unlike the membership inference case where the attacks perform dissimilar across training and test candidate sets.&lt;/li&gt;
&lt;li&gt;White-box attack outperforms even the imputation attack that has access to the class label. Thus, the model is leaking something beyond what an imputation can learn from the same data that the model is trained on. Is this because the imputation model is not learning effectively?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;jul-14-2022-vanilla-model-training-contd&#34;&gt;[Jul 14, 2022] Vanilla Model Training [Contd.]&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;Here we study the inference of &lt;em&gt;Asian&lt;/em&gt; race from Texas-100X data set to see if the trend is same as what we observed for ethnicity attribute above. For this experiment, we remove the ethnicity attribute from the model training, since it is correlated to the target race attribute as we saw above.&lt;/p&gt;

&lt;h3 id=&#34;predicting-asian-race-from-texas-100x-i&#34;&gt;Predicting &lt;em&gt;Asian&lt;/em&gt; race from Texas-100X (I)&lt;/h3&gt;

&lt;p&gt;The model is trained on general distribution and the adversary might have access to either the training set, or training distribution or a different distribution based on the adversarial knowledge. In the general distribution, 22% of records have Asian race.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Test)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Random&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;34 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;30 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;38 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;31 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;37 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;43 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;42 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;49 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;34 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;43 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;44 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;44 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;31 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;51 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;34 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36 $\pm$ 2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;45 $\pm$ 9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;49 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Table 7:&lt;/strong&gt; PPV (%) for predicting top-100 records on Texas-100X. Model is trained on general distribution.&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; We observe the same trend&amp;mdash; white-box attack does better than imputation and combining drastically improves the PPV of the combined attack on the general distribution candidate sets. Thus model is learning something useful about the distribution beyond the imputation. In the low setting, where the adversary only has access to skewed distribution, white-box attack still learns something useful, but combining with imputation does not further improve the PPV, suggesting that the attack signal is not different from imputation.&lt;/p&gt;

&lt;h3 id=&#34;predicting-asian-race-from-texas-100x-ii&#34;&gt;Predicting &lt;em&gt;Asian&lt;/em&gt; race from Texas-100X (II)&lt;/h3&gt;

&lt;p&gt;Here the model is trained on the skewed distribution that has 15% of records with Asian race.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;High (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Med (Test)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Train)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Low (Test)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Random&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15 $\pm$ 0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;26 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;16 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;31 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;30 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14 $\pm$ 1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;34 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;34 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;48 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;50 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;17 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;16 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;53 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;17 $\pm$ 1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;44 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;49 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;42 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;53 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;16 $\pm$ 7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19 $\pm$ 8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;53 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;54 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;58 $\pm$ 8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Table 8:&lt;/strong&gt; PPV (%) for predicting top-100 records on Texas-100X. Model is trained on general distribution.&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; Here also the observation is the same. White-box attack does better than imputation when the model is trained on the skewed distribution and the candidate sets are also from the skewed distribution.&lt;/p&gt;

&lt;h4 id=&#34;results-on-out-of-distribution-candidate-set-2&#34;&gt;Results on out-of-distribution candidate set&lt;/h4&gt;

&lt;p&gt;Here we evaluate the attacks on out-of-distribution candidate set. For the model trained on general distribution, the OOD set is from the skewed distribution, and vice-versa.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;High &lt;br&gt;(skewed $\mathcal{D}$)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Med &lt;br&gt;(skewed $\mathcal{D}$)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Low &lt;br&gt;(skewed $\mathcal{D}$)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;High &lt;br&gt;(general $\mathcal{D}$)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Med &lt;br&gt;(general $\mathcal{D}$)&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Low &lt;br&gt;(general $\mathcal{D}$)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Random&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21 $\pm$ 0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;17 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;16 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;31 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;26 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;17 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;16 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;23 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;30 $\pm$ 2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36 $\pm$ 8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36 $\pm$ 8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;40 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;46 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;29 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;43 $\pm$ 3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;31 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;45 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;22 $\pm$ 0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;55 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;33 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;42 $\pm$ 8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\cdot$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;50 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28 $\pm$ 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;44 $\pm$ 7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\diamondsuit$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;31 $\pm$ 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;43 $\pm$ 5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WB$\square$IP$^\dagger$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20 $\pm$ 6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;57 $\pm$ 3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;31 $\pm$ 4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;39 $\pm$ 4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;&lt;strong&gt;Table 9:&lt;/strong&gt; PPV (%) for predicting top-100 records on Texas-100X. Model is trained without Ethnicity attribute.&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Revisiting Membership Inference Under Realistic Assumptions</title>
      <link>https://bargavjayaraman.github.io/publication/revisiting-mi/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/revisiting-mi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Xbox One X HDD Loading Test</title>
      <link>https://bargavjayaraman.github.io/post/xbox_one_x_hdd_loading_test/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/post/xbox_one_x_hdd_loading_test/</guid>
      <description>

&lt;p&gt;With the advent of 4K UHD and HDR games, it is not uncommon nowadays that your Xbox One X stock HDD of 1 TB gets filled quickly with just a handful of games. And my case is no different, within one year of purchasing the console, I already ran out of hard disk space. So I did what any gamer would do in my situation; I ordered a &lt;a href=&#34;https://www.bestbuy.com/site/seagate-expansion-4tb-external-usb-3-0-portable-hard-drive-black/4820200.p?skuId=4820200&#34; target=&#34;_blank&#34;&gt;4TB external hard drive&lt;/a&gt; [Yay!]. Now that I planned to transfer some games from my internal HDD to the external HDD, curiosity plagued the researcher in me! Is the internal HDD faster at loading games than the external HDD? Or is it the other way around? Both schools of thoughts have their own reasoning. On one hand internal HDD is, well, internal. It uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Serial_ATA&#34; target=&#34;_blank&#34;&gt;SATA 3&lt;/a&gt; connection supporting upto 600 MB/s of bandwidth. On the other hand, external HDD uses &lt;a href=&#34;https://en.wikipedia.org/wiki/USB_3.0&#34; target=&#34;_blank&#34;&gt;USB 3.0&lt;/a&gt; connection supporting upto 625 MB/s of bandwidth. Both the hard drives have rotation speed of 5400 RPM, so it is still an apple to apple comparison.&lt;/p&gt;

&lt;p&gt;Note that this idea of comparing the loading speeds of internal and external HDDs is not novel. There are already many videos and blogs on this topic - be it for computers or consoles. What&amp;rsquo;s new in this blog is to evaluate this in a [&lt;em&gt;cough&lt;/em&gt;] scientific way - using hypothesis testing. Let&amp;rsquo;s get into a [somewhat] scientific evaluation!&lt;/p&gt;

&lt;h2 id=&#34;hypothesis-testing-on-game-loading-task&#34;&gt;Hypothesis testing on game loading task&lt;/h2&gt;

&lt;p&gt;Game loading time is measured (in &lt;em&gt;mm:ss&lt;/em&gt;) as the time taken to launch the game - from clicking on the game till the point where the player gets the control of the playable character, as shown in this &lt;a href=&#34;https://www.youtube.com/watch?v=MQ_pDLMI650&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;. I recorded the loading time for 10 AAA title games, with the exception of Forza Motorsport 7, where I recorded till the point where the main menu appears. Launching a race involves lot of manual selection process, which could add more variability in recording time.&lt;/p&gt;

&lt;p&gt;The null hypothesis is $H_0:$ &lt;em&gt;&amp;ldquo;game loading time of internal HDD is same as that of external HDD&amp;rdquo;&lt;/em&gt;, and the alternate hypothesis is $H_a:$ &lt;em&gt;&amp;ldquo;game loading time of internal HDD is not same as that of external HDD&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;experimental-observations&#34;&gt;Experimental observations&lt;/h2&gt;

&lt;p&gt;The game loading times are reported in the table below.
&lt;table&gt;
    &lt;tr&gt;
        &lt;th&gt;Game Title&lt;/th&gt; &lt;th&gt;Internal HDD Time &lt;br&gt;(&lt;i&gt;mm:ss&lt;/i&gt;)&lt;/th&gt; &lt;th&gt;External HDD Time &lt;br&gt;(&lt;i&gt;mm:ss&lt;/i&gt;)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Assassin&amp;rsquo;s Creed Origins&lt;/td&gt; &lt;td&gt;01:48.50&lt;/td&gt; &lt;td&gt;01:51.05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Tomb Raider&lt;/td&gt; &lt;td&gt;00:50.95&lt;/td&gt; &lt;td&gt;00:41.78&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Rise of Tomb Raider&lt;/td&gt; &lt;td&gt;02:32.14&lt;/td&gt; &lt;td&gt;02:15.54&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Shadow of Tomb Raider&lt;/td&gt; &lt;td&gt;01:36.63&lt;/td&gt; &lt;td&gt;01:24.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Halo Master Chief Collection&lt;/td&gt; &lt;td&gt;00:57.29&lt;/td&gt; &lt;td&gt;00:46.89&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Batman: Arkham Assylum&lt;/td&gt; &lt;td&gt;01:09.88&lt;/td&gt; &lt;td&gt;01:03.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Batman: Arkham City&lt;/td&gt; &lt;td&gt;01:13.94&lt;/td&gt; &lt;td&gt;01:12.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Batman: Arkham Knight&lt;/td&gt; &lt;td&gt;01:39.64&lt;/td&gt; &lt;td&gt;01:36.14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Forza Horizon 4&lt;/td&gt; &lt;td&gt;02:33.74&lt;/td&gt; &lt;td&gt;02:04.31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Forza Motorsport 7&lt;/td&gt; &lt;td&gt;01:00.00&lt;/td&gt; &lt;td&gt;00:50.69&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;Since there is a one-to-one correspondence of each game, we can use &lt;a href=&#34;https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/t-test/&#34; target=&#34;_blank&#34;&gt;paired sample T-test&lt;/a&gt;.
We can calculate the $t$-value for the paired sample T-test using a simple formula:&lt;/p&gt;

&lt;p&gt;$t-value = \frac{D_1 / N}{\sqrt{\frac{D_2 - D_1^2 / N}{N(N-1)}}}$&lt;/p&gt;

&lt;p&gt;Where $N$ is the total number of games ($N = 10$), $D_1$ is the total difference between loading times of internal and external HDDs ($D_1 = 96.71$) and $D_2$ is the total squared difference between loading times of internal and external HDDs ($D_2 = 1641.4153$). The calculated $t$-value comes out to be 3.4526 which is greater than the threshold $t$-value of 3.250 for a significance threshold of 0.005. Hence we can reject the null hypothesis $H_0$ with 99.5% confidence and conclude that [&lt;em&gt;my&lt;/em&gt;] external HDD is faster than the internal HDD of [&lt;em&gt;my&lt;/em&gt;] Xbox One X.&lt;/p&gt;

&lt;h2 id=&#34;disclaimer-on-the-validity-of-results&#34;&gt;Disclaimer on the validity of results&lt;/h2&gt;

&lt;p&gt;The above results are for a single loading of each game, and the loading time might vary by a few seconds across multiple runs. Ideally, average loading time should be used for such experiments. However, it should be noted that if the same game is loaded multiple times, the consequent loading will be faster due to &lt;em&gt;memory caching&lt;/em&gt;. To avoid caching, games should be loaded in an alternative fashion. Finally, it is important to put a disclaimer that the reported results are for my hardware, and these results &lt;em&gt;might&lt;/em&gt; vary for others. So take it with a grain of salt! :P&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Keeping the above disclaimer in mind, I conclude that my external HDD loads games faster than the stock HDD of my Xbox One X. This benefit might be attributed to the extra 25 MB/s bandwidth of USB 3.0 interface used by the external HDD.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient Privacy-Preserving Nonconvex Optimization</title>
      <link>https://bargavjayaraman.github.io/publication/nonconvex-optimization/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/nonconvex-optimization/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>https://bargavjayaraman.github.io/publication/evaluating-dpml/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/evaluating-dpml/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/JAGhqbY_U50&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization</title>
      <link>https://bargavjayaraman.github.io/publication/dp-erm/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/dp-erm/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/rwyWiDyVmjE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>https://bargavjayaraman.github.io/post/evaluating-dpml-results/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/post/evaluating-dpml-results/</guid>
      <description>

&lt;p&gt;With the recent advances in composition of differential private mechanisms, the research community has been able to achieve meaningful deep learning with privacy budgets in single digits. R&amp;#x00E8;nyi differential privacy (RDP) is one mechanism that provides tighter composition which is widely used because of its implementation in TensorFlow Privacy (recently, Gaussian differential privacy (GDP) has shown a tighter analysis for low privacy budgets, but it was not yet available when we did this work). But the central question that remains to be answered is: &lt;em&gt;how private are these methods in practice?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this blog post, we answer this question by empirically evaluating the privacy leakage of differential private neural networks via membership inference attacks. This work appeared in &lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity19&#34; target=&#34;_blank&#34;&gt;USENIX Security&amp;rsquo;19&lt;/a&gt; (full paper: &lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/evaluatingdp.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=JAGhqbY_U50&#34; target=&#34;_blank&#34;&gt;talk video&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;training-differentially-private-models&#34;&gt;Training Differentially Private Models&lt;/h2&gt;

&lt;p&gt;We train two-layer neural network models using a training procedure similar to the popular &lt;a href=&#34;https://arxiv.org/pdf/1607.00133.pdf&#34; target=&#34;_blank&#34;&gt;DPSGD&lt;/a&gt; procedure. The training and test sets consist of seperate 10,000 instances randomly sampled from the &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34; target=&#34;_blank&#34;&gt;CIFAR-100&lt;/a&gt; data set.&lt;/p&gt;

&lt;p&gt;The figure below shows the accuracy loss of private models trained with na&amp;#x00EF;ve composition (NC) and R&amp;#x00E8;nyi differential privacy (RDP) with respect to a non-private model. As expected, models trained with RDP achieve much better utility when compared to the models trained with NC. To give a comparison, models trained with RDP achieve 53% accuracy loss at \(\epsilon = 10\), whereas the models trained with NC achieve the same utility at \(\epsilon = 500\). Due to the tighter composition, RDP mechanism adds much lesser noise when compared to NC mechanism for the same privacy budget.&lt;/p&gt;

&lt;p&gt;This is great, but what about the privacy leakage?






&lt;figure&gt;

&lt;img src=&#34;img/acc_loss.jpg&#34; &gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;privacy-comes-at-a-cost&#34;&gt;Privacy comes at a cost&lt;/h2&gt;

&lt;p&gt;To estimate privacy leakage, we implement the membership inference attack of &lt;a href=&#34;https://arxiv.org/pdf/1709.01604.pdf&#34; target=&#34;_blank&#34;&gt;Yeom et al&lt;/a&gt; and use their membership advantage metric, which is given as the difference between true positive rate (TPR) and false positive rate (FPR) of detecting whether a given instance is a part of the training set. This metric lies between 0 and 1, where 0 signifies no privacy leakage.&lt;/p&gt;

&lt;p&gt;As the figure below depicts, there is a clear trade-off between privacy and utility. While the RDP mechanism achieves higher utility, it also suffers from higher privacy leakage. The attack achieves around 0.40 membership advantage score against model trained with RDP at \(\epsilon = 1000\), with a positive predictive value (PPV) of 74%. While this is less than the privacy leakage of non-private model (highlighted in the figure below), it is a significant amount of leakage. On the other hand, the model has almost no utility at lower privacy budgets where the privacy leakage is low.






&lt;figure&gt;

&lt;img src=&#34;img/priv_leak.jpg&#34; &gt;


&lt;/figure&gt;

A more interesting observation is that we only have tight theoretical worst case guarantees on membership advantage for \(\epsilon &amp;lt; 1\), at which point the models neither have any utility nor have any empirical privacy leakage. While the attacks only give a lower bound on the privacy leakage, the huge gap between the theoretical upper bound and the empirical lower bound suggests that there could be stronger attacks  in practice.&lt;/p&gt;

&lt;h2 id=&#34;leakage-is-not-random&#34;&gt;Leakage is not random&lt;/h2&gt;

&lt;p&gt;We have shown above that the membership inference attack can be effective against a model trained with RDP at \(\epsilon = 1000\). The members identified by the attacker are not due to the randomness in machine learning process. To show this, we run run the above experiment multiple times and note the fraction of members that are repeatedly identified across different runs. The figure below shows the results. The attacker is able to identify almost a quarter of the training records with more than 82% PPV across five runs. If the leakage was due to the randomness, we would have expected a trend similar to the dotted line.






&lt;figure&gt;

&lt;img src=&#34;img/multi_run.jpg&#34; &gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The differential privacy research community has come a long way to realize practical mechanisms for privacy-preserving deep learning. However, as shown in our work, we still require significant improvements to achieve meaningful utility for privacy budgets where we have strong theoretical guarantees. Concurrently, the huge gap between the empirical privacy leakage and the theoretical bounds opens the possibility for more powerful inference attacks in practice.&lt;/p&gt;

&lt;h2 id=&#34;additional-results-in-the-paper&#34;&gt;Additional Results in the Paper&lt;/h2&gt;

&lt;p&gt;While we only discussed selected results in this blog post, the &lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/evaluatingdp.pdf&#34; target=&#34;_blank&#34;&gt;full paper&lt;/a&gt; has more experimental results across different settings as listed below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Results on Purchase-100 data set, derived from &lt;a href=&#34;https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data&#34; target=&#34;_blank&#34;&gt;Kaggle&lt;/a&gt; website.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Results for logistic regression model.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Membership inference attack of &lt;a href=&#34;https://arxiv.org/pdf/1610.05820.pdf&#34; target=&#34;_blank&#34;&gt;Shokri et al.&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Attribute inference attack of &lt;a href=&#34;https://arxiv.org/pdf/1709.01604.pdf&#34; target=&#34;_blank&#34;&gt;Yeom et al&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Vague Words and Phrases in Requirements Documents in a Multilingual Environment</title>
      <link>https://bargavjayaraman.github.io/publication/vagueness/</link>
      <pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/vagueness/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Decentralized Certificate Authorities</title>
      <link>https://bargavjayaraman.github.io/publication/dca/</link>
      <pubDate>Sun, 11 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/dca/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Aggregating Private Sparse Learning Models Using Multi-Party Computation</title>
      <link>https://bargavjayaraman.github.io/publication/model-aggregation/</link>
      <pubDate>Sun, 04 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/model-aggregation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Preserving String Matching for Cloud Computing</title>
      <link>https://bargavjayaraman.github.io/publication/searchable-encryption/</link>
      <pubDate>Mon, 29 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/searchable-encryption/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://bargavjayaraman.github.io/authors/admin/</link>
      <pubDate>Thu, 29 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/authors/admin/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m a research scientist at Oracle Labs, working on privacy and security in machine learning. Prior to this, I was a post-doctoral researcher in Meta AI Research Group, where I worked on memorization evaluation in VLMs and LLMs. I obtained PhD in Computer Science from the School of Engineering and Applied Science, University of Virginia, where I was advised by &lt;a href=&#34;https://www.cs.virginia.edu/~evans/&#34; target=&#34;_blank&#34;&gt;Professor David Evans&lt;/a&gt;. My research interest is broadly in &lt;a href=&#34;https://oblivc.org/ppml/&#34; target=&#34;_blank&#34;&gt;Privacy Preserving Machine Learning&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
