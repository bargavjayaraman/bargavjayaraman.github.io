<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Differential Privacy on Bargav Jayaraman</title>
    <link>https://bargavjayaraman.github.io/tags/differential-privacy/</link>
    <description>Recent content in Differential Privacy on Bargav Jayaraman</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://bargavjayaraman.github.io/tags/differential-privacy/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>https://bargavjayaraman.github.io/publication/evaluating-dpml/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/evaluating-dpml/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/JAGhqbY_U50&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization</title>
      <link>https://bargavjayaraman.github.io/publication/dp-erm/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/publication/dp-erm/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/rwyWiDyVmjE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>https://bargavjayaraman.github.io/post/evaluating-dpml-results/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/post/evaluating-dpml-results/</guid>
      <description>

&lt;p&gt;With the recent advancements in composition of differential private mechanisms, the research community has been able to achieve meaningful deep learning with privacy budgets in single digits&lt;sup&gt;&lt;a href=&#34;#myfootnote1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The present notions of R&amp;#x00E8;nyi differential privacy (RDP) and Gaussian differential privacy (GDP)&lt;sup&gt;&lt;a href=&#34;#myfootnote2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; are considered state-of-the-art for providing tight composition. But the central question that remains to be answered is: how private are these methods in practice? In this blog post, we answer this question by empirically evaluating the privacy leakage of differential private neural networks via membership inference attacks. This work appeared in &lt;a href=&#34;https://www.usenix.org/conference/usenixsecurity19&#34; target=&#34;_blank&#34;&gt;USENIX Security&amp;rsquo;19&lt;/a&gt; (full manuscript can be found &lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/evaluatingdp.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;differential-private-training&#34;&gt;Differential private training&lt;/h2&gt;

&lt;p&gt;We train two-layer neural network models using a training procedure similar to the popular &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/2976749.2978318&#34; target=&#34;_blank&#34;&gt;DPSGD&lt;/a&gt; procedure. The training and test sets consist of seperate 10,000 instances randomly sampled from the &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34; target=&#34;_blank&#34;&gt;CIFAR-100&lt;/a&gt; data set.&lt;/p&gt;

&lt;p&gt;Figure below shows the accuracy loss of private models trained with na&amp;#x00EF;ve composition (NC) and R&amp;#x00E8;nyi differential privacy (RDP) with respect to a non-private model. As expected, model trained with RDP achieves much better utility when compared to the model trained with NC. To give a comparison, model trained with RDP achieves 53% accuracy loss at \(\epsilon = 10\), whereas the model trained with NC achieves the same utility at \(\epsilon = 500\). Due to the tighter composition, RDP mechanism adds much lesser noise when compared to NC mechanism for the same privacy budget. This is great, but what about the privacy leakage?






&lt;figure&gt;

&lt;img src=&#34;img/acc_loss.jpg&#34; &gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;privacy-comes-at-a-cost&#34;&gt;Privacy comes at a cost&lt;/h2&gt;

&lt;p&gt;For evaluating the privacy leakage, we implement the membership inference attack of &lt;a href=&#34;https://ieeexplore.ieee.org/document/8429311&#34; target=&#34;_blank&#34;&gt;Yeom et al&lt;/a&gt; and use their membership advantage metric, which is given as the difference between true positive rate (TPR) and false positive rate (FPR) of detecting whether a given instance is a part of the training set. This metric lies between 0 and 1, where 0 signifies no privacy leakage. As the figure below depicts, there is a clear trade-off between privacy and utility. While RDP mechanism achieves higher utility, it also suffers from higher privacy leakage. The attack achieves around 0.40 membership advantage score against model trained with RDP at \(\epsilon = 1000\), with a positive predictive value (PPV) of 74%. While this is lesser than the privacy leakage of non-private model (highlighted in the figure below), it is still not an acceptable amount of privacy leakage in practice. On the other hand, the model has almost no utility at lower privacy budgets where the privacy leakage is low.






&lt;figure&gt;

&lt;img src=&#34;img/priv_leak.jpg&#34; &gt;


&lt;/figure&gt;

A more interesting observation is that we only have tight theoretical worst case guarantees on membership advantage for \(\epsilon &amp;lt; 1\), at which point the models neither have any utility nor have any empirical privacy leakage. While the attacks only give a lower bound on the privacy leakage, the huge gap between the theoretical upper bound and the empirical lower bound suggests that there could be stronger attacks  in practice.&lt;/p&gt;

&lt;h2 id=&#34;leakage-is-amplified-across-multiple-runs&#34;&gt;Leakage is amplified across multiple runs&lt;/h2&gt;

&lt;p&gt;While we have shown above that the membership inference attack can be effective against a model trained with RDP at \(\epsilon = 1000\), things get worse when the attacker is allowed to run the attack multiple times. More specifically, the attacker gets more and more confident in predicting the membership of the individual records which are repeatedly identified across multiple runs. As the figure below shows, the attacker can identify almost a quarter of the training records with more than 82% PPV across five runs.






&lt;figure&gt;

&lt;img src=&#34;img/multi_run.jpg&#34; &gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The differential privacy research community has come a long way to realize practical mechanisms for deep learning. However, as shown in our work, we still require significant improvements to achieve meaningful utility for privacy budgets where we have strong theoretical guarantees. Concurrently, the huge gap between the empirical privacy leakage and the theoretical bounds opens the possibility for more powerful inference attacks in practice.&lt;/p&gt;

&lt;h2 id=&#34;what-s-more-in-the-paper&#34;&gt;What&amp;rsquo;s more in the paper&lt;/h2&gt;

&lt;p&gt;While we only discussed selected results in this blog post, the &lt;a href=&#34;https://www.cs.virginia.edu/~evans/pubs/usenix2019/evaluatingdp.pdf&#34; target=&#34;_blank&#34;&gt;full paper&lt;/a&gt; has more experimental results across different settings as listed below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Results on Purchase-100 data set, derived from &lt;a href=&#34;https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data&#34; target=&#34;_blank&#34;&gt;Kaggle&lt;/a&gt; website.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Results for logistic regression model.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Membership inference attack of &lt;a href=&#34;https://ieeexplore.ieee.org/document/7958568&#34; target=&#34;_blank&#34;&gt;Shokri et al.&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Attribute inference attack of &lt;a href=&#34;https://ieeexplore.ieee.org/document/8429311&#34; target=&#34;_blank&#34;&gt;Yeom et al&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;myfootnote1&#34;&gt;1&lt;/a&gt;: with some caveats such as model pre-training and adding less noise at each private training epoch with early stopping when the privacy budget exceeds the limit.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;myfootnote2&#34;&gt;2&lt;/a&gt;: GDP wasn&amp;rsquo;t published at the time our paper came, and hence we donot consider it in our evaluation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of Private ML Models</title>
      <link>https://bargavjayaraman.github.io/project/evaluating-dpml/</link>
      <pubDate>Wed, 22 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/project/evaluating-dpml/</guid>
      <description>&lt;p&gt;Machine learning models are being extensively trained on sensitive human data (such as pictures, videos and patient health records) and are being publicly deployed as a service. With such systems in  place, privacy of the individuals involved in the model training becomes a real concern. While differential privacy provides a solution to this problem, there is always a privacy-utility trade-off when training models privately, which is not well understood. Often the practical deployments choose the model utility over privacy, that may lead to indiscernable privacy vulnerabilities. One such example of privacy vulnerability is whether an adversary can identify a particular individual in the training data. Also, what type of individuals are more vulnerable to such attacks? This question is also directly related to the problem of fairness. In light of such vulnerabilities, what privacy parameters should the ML deployments use to mitigate the risks? Our project tries to shed light on these questions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Preserving Machine Learning</title>
      <link>https://bargavjayaraman.github.io/project/ppml/</link>
      <pubDate>Sat, 03 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://bargavjayaraman.github.io/project/ppml/</guid>
      <description>&lt;p&gt;Multi-party computation protocols provide computational security to both data and model through encrypted computations. Differential privacy, on the other hand, provides information theoretic privacy by perturbing the computations with noise. These techniques consider a different threat model and have orthogonal goals. In this project, we combine both technuqies to achieve strong security and privacy for machine learning on sensitive data.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
